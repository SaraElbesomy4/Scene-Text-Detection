{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import scipy.io as sio\n",
    "import torch.nn.functional as F\n",
    "from ctpn_model import CTPN_Model\n",
    "from ctpn_utils import gen_anchor, bbox_transfor_inv, clip_box, filter_bbox,nms, TextProposalConnectorOriented\n",
    "import config\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(image, label, new_w =None, new_h =None):\n",
    "    old_h, old_w = image.shape[:2]\n",
    "    #new_h, new_w = int(new_h), int(new_w)\n",
    "    # if both the width and height are None, then return the\n",
    "    # original image\n",
    "    if new_w is None and new_h is None:\n",
    "        new_w = old_w\n",
    "        new_h = old_h\n",
    "        \n",
    "    # check to see if the width is None\n",
    "    elif new_w is None and new_h is not None:\n",
    "        # calculate the ratio of the height and construct the\n",
    "        # dimensions\n",
    "        r = new_h / float(old_h)\n",
    "        new_w = int(old_w * r)\n",
    "    \n",
    "    # otherwise, the height is None\n",
    "    elif new_w is not None and new_h is None:\n",
    "        # calculate the ratio of the width and construct the\n",
    "        # dimensions\n",
    "        r = new_w / float(old_w)\n",
    "        new_h = int(old_h * r)\n",
    "    \n",
    "    image = cv2.resize(image, (new_w, new_h))\n",
    "    \n",
    "    label[:,0] = label[:,0] * (new_w/old_w)\n",
    "    label[:,1] = label[:,1] * (new_h/old_h)\n",
    "    label[:,2] = label[:,2] * (new_w/old_w)\n",
    "    label[:,3] = label[:,3] * (new_h/old_h)\n",
    "    label[:,4] = label[:,4] * (new_w/old_w)\n",
    "    label[:,5] = label[:,5] * (new_h/old_h)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SceneTextDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mat_paths, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mat_file (string): Path to the mat file.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.mat_paths = mat_paths\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return (1555)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "                   \n",
    "        img_name = os.path.join(self.root_dir,'img'+ str(idx) + '.jpg')\n",
    "        image = cv2.imread(img_name)\n",
    "        label_name = os.path.join(self.mat_paths, 'rect_gt_img' + str(idx)+ '.mat')                                          \n",
    "        label = sio.loadmat(label_name)\n",
    "        label = list(label.items())\n",
    "        label = np.array(label)\n",
    "        label = label[3,1]\n",
    "        label = label[:,0:6]\n",
    "        label = label.astype(float)\n",
    "        \n",
    "        transformed_image, transformed_label = rescale(image, label, new_h=720)\n",
    "        \n",
    "        return transformed_image, transformed_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and test sets and loading the two sets into dataloaders\n",
    "\n",
    "text_dataset =SceneTextDataset(mat_paths='E:/CIE/CIE 3/Spring 2020/Computer Vision/Final Project/Dataset/Groundtruth/Rectangular/All',\n",
    "                                    root_dir='E:/CIE/CIE 3/Spring 2020/Computer Vision/Final Project/Dataset/Images/All')\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(text_dataset, [1455, 100])\n",
    "\n",
    "dataloader_train = DataLoader(train_set,batch_size=1, shuffle=True,num_workers=0)\n",
    "\n",
    "dataloader_test = DataLoader(test_set,batch_size=1, shuffle=True,num_workers=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utilites functions to make the detection on the test data\n",
    "\n",
    "prob_thresh = 0.5\n",
    "gpu = True\n",
    "if not torch.cuda.is_available():\n",
    "    gpu = False\n",
    "device = torch.device('cuda:0' if gpu else 'cpu')\n",
    "weights = os.path.join(config.checkpoints_dir, 'CTPN.pth')\n",
    "model = CTPN_Model()\n",
    "model.load_state_dict(torch.load(weights, map_location=device)['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "def dis(image):\n",
    "    cv2.imshow('image', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def get_2_points(text):\n",
    "    '''\n",
    "     Objective: to drop the unused elements from the input text and return only the 2 points to compare them with the opposite 2 points in the label.\n",
    "     Input: a numpy array that retrieved from the model and represent the coordinates of the points that used to draw the detection box.\n",
    "     output: a numpy array that have only 2 points.\n",
    "    '''\n",
    "\n",
    "    text = np.delete(text, np.s_[2:6], axis=1)\n",
    "    text = text[:, 0:4]\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_4_points(label):\n",
    "    '''\n",
    "    Objective: to return the four points coordinates from the two points in the label array\n",
    "    Input: the label with shape (1,4) has 2 points: (xmin, ymin) and (xmax, ymax)\n",
    "    output: 4 points (xmin, ymin) (x1, y1) (x2, y2) (xmax, ymax)\n",
    "    '''\n",
    "    p1 = (label[:, 0], label[:, 1])\n",
    "    p2 = (label[:, 2], label[:, 1])\n",
    "    p3 = (label[:, 2], label[:, 3])\n",
    "    p4 = (label[:, 0], label[:, 3])\n",
    "   \n",
    "    return p1, p2, p3, p4\n",
    "\n",
    "def intersect(p1, p2, p3, p4, m1, m2, m3, m4):\n",
    "    '''\n",
    "     Objective: to calculate the area of the intersection between 2 rectangles.\n",
    "     Inputs: 8 tuples: 4 tuples for each rectangle represent the 4 points to draw it.\n",
    "     Output: the area of the intersection (float)\n",
    "    '''\n",
    "    box1 = Polygon([p1, p2, p3, p4])\n",
    "    box2 = Polygon([m1, m2, m3, m4])\n",
    "    intersection = box1.intersection(box2)\n",
    "    return intersection.area\n",
    "\n",
    "\n",
    "def get_iou(box_a, box_b):\n",
    "    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n",
    "    is simply the intersection over union of two boxes.  Here we operate on\n",
    "    ground truth boxes and default boxes.\n",
    "    E.g.:\n",
    "        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
    "    Args:\n",
    "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n",
    "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n",
    "    Return:\n",
    "        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n",
    "    \"\"\"\n",
    "    a1, a2, a3, a4 = get_4_points(box_a)\n",
    "    b1, b2, b3, b4 = get_4_points(box_b)\n",
    "    inter = intersect(a1, a2, a3, a4, b1, b2, b3, b4)\n",
    "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
    "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1)  # [A,B]\n",
    "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
    "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0)  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]\n",
    "\n",
    "\n",
    "def get_accuracy(label, target):\n",
    "    '''\n",
    "    Objective: get an image that contains multiple detection boxes and loop over them to get the iou accuracy of each box and then get the average accuracy\n",
    "               of all boxes to represent the accuracy for this image.\n",
    "    Input:\n",
    "           label: tensor of the groundtruth labels of one image\n",
    "           target: tensor of the label that returned by the model for one image\n",
    "    Output: a float number represents the iou accuracy of one image\n",
    "    '''\n",
    "    target_rows = target.shape[0]\n",
    "    label_rows = label.shape[0]\n",
    "    iou_list = []\n",
    "    for t in range(target_rows):\n",
    "        for L in range(label_rows):\n",
    "            iou_box = get_iou(label[L, :].unsqueeze(0), target[t, :].unsqueeze(0))\n",
    "            iou_list.append(iou_box)\n",
    "    iou_list = sorted(iou_list, reverse=True)\n",
    "    iou_list = np.asarray(iou_list)\n",
    "    final_list = iou_list[0:target_rows]\n",
    "    acc_image = sum(final_list)/len(final_list)\n",
    "\n",
    "    return acc_image\n",
    "\n",
    "\n",
    "def get_det_boxes(image,display = True, expand = True):\n",
    "    image_r = image.copy()\n",
    "    image_c = image.copy()\n",
    "    h, w = image.shape[:2]\n",
    "    image = image.astype(np.float32) - config.IMAGE_MEAN\n",
    "    image = torch.from_numpy(image.transpose(2, 0, 1)).unsqueeze(0).float()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        cls, regr = model(image)\n",
    "        cls_prob = F.softmax(cls, dim=-1).cpu().numpy()\n",
    "        regr = regr.cpu().numpy()\n",
    "        anchor = gen_anchor((int(h / 16), int(w / 16)), 16)\n",
    "        bbox = bbox_transfor_inv(anchor, regr)\n",
    "        bbox = clip_box(bbox, [h, w])\n",
    "        # print(bbox.shape)\n",
    "\n",
    "        fg = np.where(cls_prob[0, :, 1] > prob_thresh)[0]\n",
    "        # print(np.max(cls_prob[0, :, 1]))\n",
    "        select_anchor = bbox[fg, :]\n",
    "        select_score = cls_prob[0, fg, 1]\n",
    "        select_anchor = select_anchor.astype(np.int32)\n",
    "        # print(select_anchor.shape)\n",
    "        keep_index = filter_bbox(select_anchor, 16)\n",
    "\n",
    "        # nms\n",
    "        select_anchor = select_anchor[keep_index]\n",
    "        select_score = select_score[keep_index]\n",
    "        select_score = np.reshape(select_score, (select_score.shape[0], 1))\n",
    "        nmsbox = np.hstack((select_anchor, select_score))\n",
    "        keep = nms(nmsbox, 0.3)\n",
    "        # print(keep)\n",
    "        select_anchor = select_anchor[keep]\n",
    "        select_score = select_score[keep]\n",
    "\n",
    "        # text line-\n",
    "        textConn = TextProposalConnectorOriented()\n",
    "        text = textConn.get_text_lines(select_anchor, select_score, [h, w])\n",
    "\n",
    "        # expand text\n",
    "\n",
    "        if expand:\n",
    "            for idx in range(len(text)):\n",
    "                text[idx][0] = max(text[idx][0] - 10, 0)\n",
    "                text[idx][2] = min(text[idx][2] + 10, w - 1)\n",
    "                text[idx][4] = max(text[idx][4] - 10, 0)\n",
    "                text[idx][6] = min(text[idx][6] + 10, w - 1)\n",
    "\n",
    "\n",
    "\n",
    "        # print(text)\n",
    "        if display:\n",
    "\n",
    "            blank = np.zeros(image_c.shape,dtype=np.uint8)\n",
    "            for box in select_anchor:\n",
    "                pt1 = (box[0], box[1])\n",
    "                pt2 = (box[2], box[3])\n",
    "                blank = cv2.rectangle(blank, pt1, pt2, (50, 0, 0), -1)\n",
    "            image_c = image_c+blank\n",
    "            image_c[image_c>255] = 255\n",
    "\n",
    "\n",
    "            for i in text:\n",
    "                s = str(round(i[-1] * 100, 2)) + '%'\n",
    "                i = [int(j) for j in i]\n",
    "                cv2.line(image_c, (i[0], i[1]), (i[2], i[3]), (0, 0, 255), 2)\n",
    "                cv2.line(image_c, (i[0], i[1]), (i[4], i[5]), (0, 0, 255), 2)\n",
    "                cv2.line(image_c, (i[6], i[7]), (i[2], i[3]), (0, 0, 255), 2)\n",
    "                cv2.line(image_c, (i[4], i[5]), (i[6], i[7]), (0, 0, 255), 2)\n",
    "                cv2.putText(image_c, s, (i[0]+13, i[1]+13),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            1,\n",
    "                            (255,0,0),\n",
    "                            2,\n",
    "                            cv2.LINE_AA)\n",
    "            # dis(image_c)\n",
    "\n",
    "\n",
    "        return text, image_c, image_r                   #text[:,1:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 9)\n",
      "1\n",
      "(4, 9)\n",
      "2\n",
      "(5, 9)\n",
      "3\n",
      "(5, 9)\n",
      "4\n",
      "(2, 9)\n",
      "5\n",
      "(5, 9)\n",
      "6\n",
      "(5, 9)\n",
      "7\n",
      "(0, 9)\n",
      "8\n",
      "(0, 9)\n",
      "9\n",
      "(4, 9)\n",
      "10\n",
      "(3, 9)\n",
      "11\n",
      "(2, 9)\n",
      "12\n",
      "(2, 9)\n",
      "13\n",
      "(2, 9)\n",
      "14\n",
      "(3, 9)\n",
      "15\n",
      "(11, 9)\n",
      "16\n",
      "(9, 9)\n",
      "17\n",
      "(4, 9)\n",
      "18\n",
      "(4, 9)\n",
      "19\n",
      "(2, 9)\n",
      "20\n",
      "(4, 9)\n",
      "21\n",
      "(2, 9)\n",
      "22\n",
      "(3, 9)\n",
      "23\n",
      "(7, 9)\n",
      "24\n",
      "(0, 9)\n",
      "25\n",
      "(1, 9)\n",
      "26\n",
      "(2, 9)\n",
      "27\n",
      "(0, 9)\n",
      "28\n",
      "(1, 9)\n",
      "29\n",
      "(2, 9)\n",
      "30\n",
      "(7, 9)\n",
      "31\n",
      "(4, 9)\n",
      "32\n",
      "(2, 9)\n",
      "33\n",
      "(2, 9)\n",
      "34\n",
      "(2, 9)\n",
      "35\n",
      "(0, 9)\n",
      "36\n",
      "(2, 9)\n",
      "37\n",
      "(1, 9)\n",
      "38\n",
      "(2, 9)\n",
      "39\n",
      "(1, 9)\n",
      "40\n",
      "(5, 9)\n",
      "41\n",
      "(5, 9)\n",
      "42\n",
      "(1, 9)\n",
      "43\n",
      "(6, 9)\n",
      "44\n",
      "(1, 9)\n",
      "45\n",
      "(2, 9)\n",
      "46\n",
      "(1, 9)\n",
      "47\n",
      "(1, 9)\n",
      "48\n",
      "(3, 9)\n",
      "49\n",
      "(1, 9)\n",
      "50\n",
      "(2, 9)\n",
      "51\n",
      "(1, 9)\n",
      "52\n",
      "(0, 9)\n",
      "53\n",
      "(4, 9)\n",
      "54\n",
      "(1, 9)\n",
      "55\n",
      "(2, 9)\n",
      "56\n",
      "(2, 9)\n",
      "57\n",
      "(10, 9)\n",
      "58\n",
      "(2, 9)\n",
      "59\n",
      "(2, 9)\n",
      "60\n",
      "(2, 9)\n",
      "61\n",
      "(5, 9)\n",
      "62\n",
      "(3, 9)\n",
      "63\n",
      "(1, 9)\n",
      "64\n",
      "(2, 9)\n",
      "65\n",
      "(6, 9)\n",
      "66\n",
      "(6, 9)\n",
      "67\n",
      "(2, 9)\n",
      "68\n",
      "(4, 9)\n",
      "69\n",
      "(0, 9)\n",
      "70\n",
      "(6, 9)\n",
      "71\n",
      "(1, 9)\n",
      "72\n",
      "(5, 9)\n",
      "73\n",
      "(4, 9)\n",
      "74\n",
      "(3, 9)\n",
      "75\n",
      "(1, 9)\n",
      "76\n",
      "(5, 9)\n",
      "77\n",
      "(1, 9)\n",
      "78\n",
      "(3, 9)\n",
      "79\n",
      "(3, 9)\n",
      "80\n",
      "(1, 9)\n",
      "81\n",
      "(6, 9)\n",
      "82\n",
      "(3, 9)\n",
      "83\n",
      "(0, 9)\n",
      "84\n",
      "(2, 9)\n",
      "85\n",
      "(2, 9)\n",
      "86\n",
      "(5, 9)\n",
      "87\n",
      "(1, 9)\n",
      "88\n",
      "(1, 9)\n",
      "89\n",
      "(0, 9)\n",
      "90\n",
      "(4, 9)\n",
      "91\n",
      "(6, 9)\n",
      "92\n",
      "(6, 9)\n",
      "93\n",
      "(2, 9)\n",
      "94\n",
      "(3, 9)\n",
      "95\n",
      "(0, 9)\n",
      "96\n",
      "(10, 9)\n",
      "97\n",
      "(0, 9)\n",
      "98\n",
      "(6, 9)\n",
      "99\n",
      "(2, 9)\n",
      "100\n",
      "0.518352824059099\n"
     ]
    }
   ],
   "source": [
    "# The driver code to test the test data and get the iou accuracy\n",
    "if __name__ == '__main__':\n",
    "    all_acc_list = []\n",
    "    c = 0\n",
    "    for image, label in dataloader_test:\n",
    "      \n",
    "        image = np.asarray(image).squeeze(0)\n",
    "        label = np.asarray(label).squeeze(0)\n",
    "        label = label[:, 0:4]\n",
    "        text, image_c, image_r = get_det_boxes(image)\n",
    "        print(text.shape)\n",
    "        if len(text) == 0:\n",
    "            image, label = next(iter(dataloader_test))\n",
    "        else:\n",
    "            text = get_2_points(text)\n",
    "            text = torch.from_numpy(text)\n",
    "            label = torch.from_numpy(label)\n",
    "            #dis(image_c)\n",
    "            acc = get_accuracy(label, text)\n",
    "                \n",
    "        c += 1\n",
    "        print(c)\n",
    "    \n",
    "    all_acc_list.append(acc)    \n",
    "    test_acc = sum(all_acc_list)/len(all_acc_list)\n",
    "    print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
